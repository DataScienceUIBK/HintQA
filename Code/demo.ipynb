{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d67a6d4f",
      "metadata": {
        "id": "d67a6d4f"
      },
      "source": [
        "\n",
        "# HintQA Pipeline\n",
        "\n",
        "This tutorial demonstrates how to implement a **RAG (Retrieval-Augmented Generation)** pipeline using hints as context.\n",
        "RAG combines the strengths of both a retrieval mechanism and a generative model to produce more accurate and\n",
        "contextually relevant responses. In this pipeline, the system retrieves hints related to a question and uses them to\n",
        "help guide a large language model (LLM) in generating more precise answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a40b992",
      "metadata": {
        "id": "7a40b992"
      },
      "source": [
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Installing HintEval**: Install HintEval library to generate hints.\n",
        "2. **Generating a dataset**: Create a dataset with questions and answers.\n",
        "3. **Generating hints**: Use a model to generate hints that will later be retrieved in the RAG process.\n",
        "4. **RAG Pipeline**: Use hints in the retrieval process to help the LLM generate accurate answers.\n",
        "5. **Displaying results**: Display the predicted answers alongside the actual (ground truth) answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8999154f",
      "metadata": {
        "id": "8999154f"
      },
      "source": [
        "> We assume you have an active API key for the TogetherAI platform and are using this platform for hint generation using LLM. In this example, we use *meta-llama/Llama-3.3-70B-Instruct-Turbo* as the model, which is available on the TogetherAI platform. If you wish to use another platform, ensure the model name is valid for that platform.\n",
        "\n",
        "> The output may vary from the example shown below due to the inherent non-deterministic nature of large language models. :::\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install hinteval\n",
        "!pip install httpx==0.27.2\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "5Uytj9Co4TXk"
      },
      "id": "5Uytj9Co4TXk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pipeline Argument\n",
        "\n",
        "api_key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' # @param {type:\"string\"}\n",
        "base_url = 'https://api.together.xyz/v1' # @param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Bzd3r4AS2h3U"
      },
      "id": "Bzd3r4AS2h3U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e7ca9eff",
      "metadata": {
        "id": "e7ca9eff"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting the pipeline, ensure you have the necessary libraries and imports for dataset preparation, hint generation, and LLM interaction."
      ],
      "metadata": {
        "id": "XZGnwGwT8iku"
      },
      "id": "XZGnwGwT8iku"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe895e1",
      "metadata": {
        "id": "afe895e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from prettytable import PrettyTable\n",
        "from openai import OpenAI\n",
        "from hinteval import Dataset\n",
        "from hinteval.cores import Instance, Subset\n",
        "from hinteval.model import AnswerAgnostic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These imports are essential for the various tasks within the pipeline:\n",
        "- **`os`**: Used for setting environment variables, such as checkpoint directories.\n",
        "- **`random`**: For randomness operations like selecting few-shot examples.\n",
        "- **`tqdm`**: Provides progress bars for loops, improving visibility of task completion.\n",
        "- **`PrettyTable`**: Used to display the final comparison of predicted and ground truth answers in a readable format.\n",
        "- **`openai.OpenAI`**: For interacting with the OpenAI API to retrieve model completions.\n",
        "- **`hinteval.Dataset`, `Instance`, `Subset`**: These classes manage dataset loading, creation, and instance handling.\n",
        "- **`hinteval.model.AnswerAgnostic`**: The class responsible for generating answer-agnostic hints based on questions."
      ],
      "metadata": {
        "id": "__OrAW_t8zSq"
      },
      "id": "__OrAW_t8zSq"
    },
    {
      "cell_type": "markdown",
      "id": "c761ab7f",
      "metadata": {
        "id": "c761ab7f"
      },
      "source": [
        "## 1. Generating a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first step, we create a dataset containing questions and corresponding answers. At this stage, no hints are included in the dataset, which will be added later.\n"
      ],
      "metadata": {
        "id": "_s_ZYdTZ85IK"
      },
      "id": "_s_ZYdTZ85IK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9f6624",
      "metadata": {
        "id": "1a9f6624"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generating_dataset():\n",
        "    dataset = Dataset(name='my_dataset')\n",
        "    dataset.add_subset(Subset('entire'))\n",
        "    kg_dataset = Dataset.download_and_load_dataset('KG-Hint')\n",
        "    kg_instances_partial = kg_dataset['entire'].get_instances()[10:30]\n",
        "    for instance in kg_instances_partial:\n",
        "        new_instance = Instance(question=instance.question, answers=instance.answers, hints=[])\n",
        "        dataset['entire'].add_instance(new_instance)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dataset creation**: The function creates a new dataset named `my_dataset` and adds a subset called `entire`.\n",
        "- **KG-Hint dataset**: It loads an existing dataset (KG-Hint) that contains questions, answers, and hints. The tutorial selects a partial subset of instances (from index 10 to 30) to work with.\n",
        "- **New instances**: Each selected instance consists of a question and its answers. Hints are initially set as empty lists and will be populated in the next step.\n"
      ],
      "metadata": {
        "id": "HWiSVcbZ87XN"
      },
      "id": "HWiSVcbZ87XN"
    },
    {
      "cell_type": "markdown",
      "id": "41d8888d",
      "metadata": {
        "id": "41d8888d"
      },
      "source": [
        "## 2. Generating Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, hints are generated for the dataset. These hints will later serve as part of the information retrieved in the RAG process to guide the LLM in generating answers.\n"
      ],
      "metadata": {
        "id": "DghzRz1g8-4R"
      },
      "id": "DghzRz1g8-4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bb5018",
      "metadata": {
        "id": "d1bb5018"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_hints(dataset: Dataset):\n",
        "    answer_agnostic = AnswerAgnostic('meta-llama/Llama-3.3-70B-Instruct-Turbo', api_key=api_key, batch_size=5,\n",
        "                                     checkpoint=True, enable_tqdm=True)\n",
        "    dataset_instances = dataset['entire'].get_instances()\n",
        "    answer_agnostic.generate(dataset_instances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **AnswerAgnostic model**: We use a pre-trained model (`meta-llama/Llama-3.3-70B-Instruct-Turbo`) to generate hints in answer agnostic scenario.\n",
        "- **Generate hints**: The function `generate` processes the dataset instances and generates hints, which are then associated with each question. These hints will be stored and used during the RAG pipeline.\n"
      ],
      "metadata": {
        "id": "dc20g8wN9CaS"
      },
      "id": "dc20g8wN9CaS"
    },
    {
      "cell_type": "markdown",
      "id": "42e9cc85",
      "metadata": {
        "id": "42e9cc85"
      },
      "source": [
        "## 3. Generating Prompts with Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step involves creating a prompt for the LLM by combining the question with the generated hints. The hints act as context that helps the model generate a more accurate answer.\n"
      ],
      "metadata": {
        "id": "4rp97jtQ9G_M"
      },
      "id": "4rp97jtQ9G_M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0989ab0c",
      "metadata": {
        "id": "0989ab0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_prompt(question, context):\n",
        "    return f\"\"\"Based on the context, answer the following question:\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Generate prompt**: This function formats the input to be passed to the LLM. It clearly separates the generated hints (context) from the question, making it easier for the model to understand the context and generate an appropriate answer.\n"
      ],
      "metadata": {
        "id": "yJsOvXS19JM0"
      },
      "id": "yJsOvXS19JM0"
    },
    {
      "cell_type": "markdown",
      "id": "696a48b2",
      "metadata": {
        "id": "696a48b2"
      },
      "source": [
        "## 4. Few-Shot Learning with Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot learning is used to provide the model with a small number of example interactions (question-hints-answer triples). This helps improve the model's performance by showing it how to handle similar cases.\n"
      ],
      "metadata": {
        "id": "lV8N0xbH9MN7"
      },
      "id": "lV8N0xbH9MN7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d791b63e",
      "metadata": {
        "id": "d791b63e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_shots(dataset: Dataset, num_of_shots=5):\n",
        "    examples = []\n",
        "    example_shots = random.sample(dataset['entire'].get_instances(), num_of_shots)\n",
        "\n",
        "    for example in example_shots:\n",
        "        example_question = example.question.question\n",
        "        example_context = '\\n'.join([hint.hint for hint in example.hints])\n",
        "        example_answer = example.answers[0].answer\n",
        "\n",
        "        prompt = generate_prompt(example_question, example_context)\n",
        "        examples.append({\"role\": \"user\", \"content\": prompt})\n",
        "        examples.append({\"role\": \"assistant\", \"content\": example_answer})\n",
        "\n",
        "    return examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Random selection**: This function selects a random subset of instances from the dataset to serve as few-shot learning examples.\n",
        "- **Example formatting**: For each example, the function retrieves the question, the associated hints (context), and the correct answer. It then generates a structured prompt to guide the model and appends the expected answer.\n"
      ],
      "metadata": {
        "id": "ZB2KxcCJ9OVT"
      },
      "id": "ZB2KxcCJ9OVT"
    },
    {
      "cell_type": "markdown",
      "id": "2f6f7d8c",
      "metadata": {
        "id": "2f6f7d8c"
      },
      "source": [
        "## 5. RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG pipeline is the core of this tutorial. In this step, the model retrieves the hints for each question, formats them into a prompt, and then uses the generative model to produce an answer. The predicted answers are then compared with the actual answers.\n"
      ],
      "metadata": {
        "id": "1bqsoFBd9WCh"
      },
      "id": "1bqsoFBd9WCh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7760b2c0",
      "metadata": {
        "id": "7760b2c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rag(dataset: Dataset):\n",
        "    system_prompt = \"You are a precise answering assistant. Always provide the shortest possible answer to factoid questions. Avoid explanations or additional details.\"\n",
        "    model_name = 'meta-llama/Llama-3.3-70B-Instruct-Turbo'\n",
        "    pipeline = OpenAI(base_url=base_url, api_key=api_key)\n",
        "\n",
        "    predicted_answers = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for instance in tqdm(dataset['entire'].get_instances(), desc='Rag pipeline'):\n",
        "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        example_shots = generate_shots(dataset, 5)\n",
        "        messages.extend(example_shots)\n",
        "\n",
        "        instance_question = instance.question.question\n",
        "        instance_context = '\\n'.join([hint.hint for hint in instance.hints])\n",
        "        instance_answer = instance.answers[0].answer\n",
        "\n",
        "        prompt = generate_prompt(instance_question, instance_context)\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        answer = pipeline.chat.completions.create(model=model_name, messages=messages)\n",
        "        predicted_answer = answer.choices[0].message.content.strip()\n",
        "\n",
        "        predicted_answers.append(predicted_answer)\n",
        "        ground_truths.append(instance_answer)\n",
        "    return predicted_answers, ground_truths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **System prompt**: This prompt defines the system's behavior, instructing it to retrieve relevant information and generate an accurate answer.\n",
        "- **Few-shot learning**: The model uses example question-hints-answer triples as few-shot learning inputs to guide its responses.\n",
        "- **Main loop**: For each instance, the model retrieves the question and associated hints (context), combines them into a prompt, and generates an answer using the LLM.\n",
        "- **Result collection**: The predicted answers and ground truth answers are stored for comparison."
      ],
      "metadata": {
        "id": "_rhBJA9O9awB"
      },
      "id": "_rhBJA9O9awB"
    },
    {
      "cell_type": "markdown",
      "id": "4333018f",
      "metadata": {
        "id": "4333018f"
      },
      "source": [
        "## 6. Displaying the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the RAG pipeline completes, the predicted answers and actual answers are displayed side by side in a table for easy comparison.\n"
      ],
      "metadata": {
        "id": "sakLJS4O9cvo"
      },
      "id": "sakLJS4O9cvo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21470a1",
      "metadata": {
        "id": "c21470a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_result_table(predicted_answers, correct_answers):\n",
        "    table = PrettyTable(['Predicted', 'Ground Truth'])\n",
        "    for predicted, ground in zip(predicted_answers, correct_answers):\n",
        "        table.add_row([predicted, ground])\n",
        "    print()\n",
        "    print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **PrettyTable**: This function uses the PrettyTable library to create a structured table showing the predicted and actual answers side by side.\n"
      ],
      "metadata": {
        "id": "gL-96Of69eYw"
      },
      "id": "gL-96Of69eYw"
    },
    {
      "cell_type": "markdown",
      "id": "ae23be7d",
      "metadata": {
        "id": "ae23be7d"
      },
      "source": [
        "## 7. Running the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the complete pipeline is executed. The steps include dataset generation, hint generation, running the RAG pipeline to answer questions, and displaying the results.\n"
      ],
      "metadata": {
        "id": "AJH_SnKA9g5B"
      },
      "id": "AJH_SnKA9g5B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f9c435",
      "metadata": {
        "id": "70f9c435"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pipeline():\n",
        "    dataset = generating_dataset()\n",
        "    generate_hints(dataset)\n",
        "    predicted_answers, ground_truths = rag(dataset)\n",
        "    print_result_table(predicted_answers, ground_truths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **End-to-end pipeline**: This function orchestrates the entire process, from dataset creation to result display. It runs through the pipeline steps in sequence, generating the dataset, producing hints, and running the RAG pipeline.\n"
      ],
      "metadata": {
        "id": "iCQPGxkM9kIM"
      },
      "id": "iCQPGxkM9kIM"
    },
    {
      "cell_type": "markdown",
      "id": "7aed4817",
      "metadata": {
        "id": "7aed4817"
      },
      "source": [
        "## 8. Main Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `main` serves as the entry point for running the entire pipeline. In this block, we define the necessary configurations, such as checkpoints and random seeds.\n"
      ],
      "metadata": {
        "id": "g94vYnP79mQM"
      },
      "id": "g94vYnP79mQM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118fbe01",
      "metadata": {
        "id": "118fbe01"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    os.environ['HINTEVAL_CHECKPOINT_DIR'] = './rag_checkpoint'\n",
        "    random.seed(1234)\n",
        "\n",
        "    pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Checkpoint Directory**:  `os.environ['HINTEVAL_CHECKPOINT_DIR'] = './rag_checkpoint'`  \n",
        "   The environment variable `HINTEVAL_CHECKPOINT_DIR` is set to define the directory where the model checkpoints are stored. Checkpoints allow the pipeline to save progress, ensuring that long-running processes can be resumed later if necessary. Here, the checkpoints are saved in the local directory `./rag_checkpoint`.\n",
        "\n",
        "- **Random Seed**:  `random.seed(1234)`  \n",
        "   Setting a seed for randomness ensures reproducibility. By fixing the seed value (in this case, 1234), the random selections made in the pipeline (e.g., for few-shot learning or data sampling) will always produce the same results when the code is re-run, making it easier to debug and compare experiments.\n",
        "\n",
        "- **Pipeline Execution**:  `pipeline()`  \n",
        "   This function call runs the entire pipeline, starting from dataset generation, hint generation, running the RAG process, and displaying the results. The pipeline is fully orchestrated and ready for execution once the script is run.\n"
      ],
      "metadata": {
        "id": "4EMRCsho9rrD"
      },
      "id": "4EMRCsho9rrD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Output\n",
        "\n",
        "```\n",
        "Checkpoint successfully reloaded for Model-AnswerAgnostic from: /content/rag_checkpoint/answer_agnostic_meta-llama_Llama-3.3-70B-Instruct-Turbo.pickle\n",
        "Generating hints using meta-llama/Llama-3.3-70B-Instruct-Turbo: 100%|██████████| 4/4 [00:00<00:00, 16100.98it/s]\n",
        "Rag pipeline: 100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
        "\n",
        "+----------------------------+------------------------+\n",
        "|         Predicted          |      Ground Truth      |\n",
        "+----------------------------+------------------------+\n",
        "|           1985             |          1984          |\n",
        "|    Niccoló Machiavelli     |  Niccoló Machiavelli   |\n",
        "|           Paris            |         Paris          |\n",
        "|           1943             |          1943          |\n",
        "|       Drew Weissman        |       Elan Musk        |\n",
        "|         Thailand           |        Thailand        |\n",
        "|           1877             |          1877          |\n",
        "|        Jeff Bezos          |       Bill Gates       |\n",
        "|           China            |        Morocco         |\n",
        "|           1933             |          1937          |\n",
        "|        Marie Curie         | Marie Skłodowska-Curie |\n",
        "|           Bonn             |         Berlin         |\n",
        "|           1971             |          1971          |\n",
        "|      Matt Groening         |        Gröning         |\n",
        "|        Louisiana           |        Alabama         |\n",
        "|           1962             |          1962          |\n",
        "|      Angela Merkel         |       Jef Bezos        |\n",
        "|          Mexico            |         Mexico         |\n",
        "|           1971             |          1970          |\n",
        "|       Bill Clinton         |      Bill Clinton      |\n",
        "+----------------------------+------------------------+\n",
        "```\n",
        "\n",
        "The table compares the predicted answers with the ground truth answers, showing the performance of the RAG pipeline. The predicted answers are generated by retrieving the relevant hints for each question and using them to guide the model's responses."
      ],
      "metadata": {
        "id": "BQDxarv99xIi"
      },
      "id": "BQDxarv99xIi"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}